import requests
from bs4 import BeautifulSoup
import json
import os       # Th√™m os
import sys      # Th√™m sys
import time
import datetime # Th∆∞ vi·ªán m·ªõi ƒë·ªÉ x·ª≠ l√Ω ng√†y gi·ªù

# --- C·∫§U H√åNH C·ªê ƒê·ªäNH ---
VNEXPRESS_URL = 'https://vnexpress.net/the-gioi'
TUOI_TRE_URL = 'https://www.24h.com.vn/tin-tuc-quoc-te-c415.html'
KEYWORDS = ['nga', 'ukraine']
STATE_FILE = 'processed_links.json'

# --- L·∫§Y B√ç M·∫¨T T·ª™ GITHUB (Thay v√¨ d√°n key) ---
try:
    BOT_TOKEN = os.environ['BOT_TOKEN']
    CHAT_ID = os.environ['CHAT_ID']
except KeyError:
    print("L·ªói: Kh√¥ng t√¨m th·∫•y BOT_TOKEN ho·∫∑c CHAT_ID.")
    print("H√£y ƒë·∫£m b·∫£o ƒë√£ set Secrets trong GitHub Actions.")
    sys.exit(1) # D·ª´ng ch∆∞∆°ng tr√¨nh n·∫øu kh√¥ng c√≥ key

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'
}

# --- C√ÅC H√ÄM CH·ª®C NƒÇNG (Gi·ªØ nguy√™n) ---

def load_processed_links():
    """
    T·∫£i links ƒë√£ x·ª≠ l√Ω. (Phi√™n b·∫£n "Nh·ªõ Vƒ©nh C·ª≠u")
    """
    try:
        with open(STATE_FILE, 'r') as f:
            processed_list = json.load(f)
            print(f"ƒê√£ t·∫£i {len(processed_list)} links t·ª´ b·ªô nh·ªõ vƒ©nh c·ª≠u.")
            return set(processed_list)
            
    except (FileNotFoundError, json.JSONDecodeError):
        # N·∫øu file kh√¥ng t·ªìn t·∫°i ho·∫∑c r·ªóng, tr·∫£ v·ªÅ set r·ªóng
        print(f"Kh√¥ng t√¨m th·∫•y file {STATE_FILE} ho·∫∑c file r·ªóng. B·∫Øt ƒë·∫ßu b·ªô nh·ªõ m·ªõi.")
        return set()

def save_processed_links(links_set):
    with open(STATE_FILE, 'w') as f:
        json.dump(list(links_set), f, indent=2)
    print(f"\nƒê√£ l∆∞u {len(links_set)} links v√†o {STATE_FILE}")

# --- H√ÄM SEND_TELEGRAM (ƒê√É N√ÇNG C·∫§P) ---
def send_telegram_message(text):
    """G·ª≠i tin (N√ÇNG C·∫§P: T·ª± ƒë·ªông x·ª≠ l√Ω l·ªói 429 rate limit)"""
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        'chat_id': CHAT_ID,
        'text': text,
        'parse_mode': 'HTML'
    }
    
    # Th·ª≠ g·ª≠i, t·ªëi ƒëa 5 l·∫ßn
    for i in range(5): 
        try:
            response = requests.post(url, data=payload, timeout=20)
            
            if response.status_code == 200:
                print("G·ª≠i tin nh·∫Øn th√†nh c√¥ng!")
                return # G·ª≠i th√†nh c√¥ng, tho√°t h√†m
                
            elif response.status_code == 429:
                # B·ªã rate limit
                error_data = response.json()
                # L·∫•y tg retry, m·∫∑c ƒë·ªãnh 5s n·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c
                retry_after = error_data.get('parameters', {}).get('retry_after', 5) 
                
                print(f"L·ªñI 429: B·ªã rate limit. T·ª± ƒë·ªông ch·ªù {retry_after + 1} gi√¢y...")
                time.sleep(retry_after + 1) # Ch·ªù v√† v√≤ng l·∫∑p s·∫Ω th·ª≠ l·∫°i
                
            else:
                # L·ªói kh√°c (400, 404, 500...)
                print(f"L·ªñI l·∫° khi g·ª≠i tin: {response.status_code} - {response.text}")
                return # L·ªói l·∫°, kh√¥ng th·ª≠ l·∫°i
                
        except Exception as e:
            print(f"L·ªñI ngo·∫°i l·ªá khi g·ª≠i tin: {e}")
            time.sleep(5) # Ngh·ªâ 5s n·∫øu c√≥ l·ªói m·∫°ng
    
    print(f"L·ªñI: Kh√¥ng th·ªÉ g·ª≠i tin nh·∫Øn sau 5 l·∫ßn th·ª≠.")

# --- C√°c h√†m scrape (KH√îNG THAY ƒê·ªîI) ---

def scrape_vnexpress():
    print("ƒêang l·∫•y tin t·ª´ VnExpress...")
    articles = []
    try:
        response = requests.get(VNEXPRESS_URL, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.select('article.item-news')
        for item in items.copy():
            title_tag = item.select_one('h3.title-news a')
            if title_tag:
                title = title_tag.get_text(strip=True)
                link = title_tag['href']
                articles.append({'title': title, 'link': link, 'source': 'VnExpress'})
    except Exception as e:
        print(f"L·ªói khi scrape VnExpress: {e}")
    return articles

def scrape_24h():
    print("ƒêang l·∫•y tin t·ª´ 24h.com.vn (Ph∆∞∆°ng ph√°p URL)...")
    articles = []
    base_url = "https://www.24h.com.vn"
    try:
        response = requests.get(TUOI_TRE_URL, headers=HEADERS, timeout=15)
        response.encoding = 'utf-8' 
        soup = BeautifulSoup(response.text, 'html.parser')
        all_links = soup.select('a')
        found_links = set()
        for link_tag in all_links:
            if not link_tag.has_attr('href'):
                continue
            link = link_tag['href']
            if "-c415a" in link and ".html" in link and link not in found_links:
                title = link_tag.get_text(strip=True)
                if not title or len(title) < 15:
                    continue
                if not link.startswith('http'):
                    link = base_url + link
                articles.append({'title': title, 'link': link, 'source': '24h.com.vn'})
                found_links.add(link) 
    except Exception as e:
        print(f"L·ªói khi scrape 24h.com.vn: {e}")
    print(f"T√¨m th·∫•y {len(articles)} b√†i t·ª´ 24h.com.vn.")
    return articles

# --- H√ÄM CH·∫†Y CH√çNH (Gi·ªØ nguy√™n) ---

def main():
    print("B·∫Øt ƒë·∫ßu chu tr√¨nh ch·∫°y...")

    now = datetime.datetime.now()
    hashtag = f"#{now.strftime('%d_%m_%Y_%H')}h" 
    print(f"Hashtag cho l·∫ßn ch·∫°y n√†y: {hashtag}")

    processed_links = load_processed_links()
    print(f"ƒê√£ t·∫£i {len(processed_links)} links ƒë√£ x·ª≠ l√Ω (c·ªßa h√¥m nay).")

    all_articles = scrape_vnexpress() + scrape_24h()
    print(f"T√¨m th·∫•y t·ªïng c·ªông {len(all_articles)} b√†i b√°o.")

    new_articles_to_send = []
    new_links_to_save = set(processed_links) 

    for article in all_articles:
        if article['link'] not in processed_links:
            new_links_to_save.add(article['link'])
            title_lower = article['title'].lower()
            if any(keyword.lower() in title_lower for keyword in KEYWORDS):
                print(f"[PH√ÅT HI·ªÜN] {article['title']}")
                new_articles_to_send.append(article)
                
    if not new_articles_to_send:
        print("Kh√¥ng c√≥ b√†i b√°o m·ªõi n√†o ch·ª©a t·ª´ kh√≥a.")
    else:
        print(f"T√¨m th·∫•y {len(new_articles_to_send)} b√†i m·ªõi, ƒëang g·ª≠i th√¥ng b√°o...")
        for article in reversed(new_articles_to_send):
            message = (
                f"üì∞ <b>{article['source']} - Tin t·ª©c m·ªõi</b>\n\n"
                f"<b>{article['title']}</b>\n\n"
                f"{article['link']}\n\n"
                f"<i>{hashtag}</i>" 
            )
            send_telegram_message(message)
            # Ch√∫ng ta v·∫´n gi·ªØ 1s ngh·ªâ "l·ªãch s·ª±" gi·ªØa c√°c tin
            time.sleep(1) 
            
    save_processed_links(new_links_to_save)
    print("Ho√†n t·∫•t chu tr√¨nh.")

if __name__ == "__main__":
    main()